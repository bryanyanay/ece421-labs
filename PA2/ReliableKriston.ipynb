{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def fit_NeuralNetwork(X_train,y_train,alpha,hidden_layer_sizes,epochs):\n",
    "    # Initialize the epoch errors\n",
    "    err=np.zeros((epochs,1))\n",
    "    \n",
    "    # Initialize the architecture\n",
    "    N, d = X_train.shape\n",
    "    X0 = np.ones((N,1))\n",
    "    X_train = np.hstack((X0,X_train))\n",
    "    d = d + 1\n",
    "    L = len(hidden_layer_sizes)\n",
    "    L = L + 2 # L is the total layer = hidden layer + 2\n",
    "    \n",
    "    #Initializing the weights for input layer, separately initialize this one\n",
    "    weight_layer = np.random.normal(0, 0.1, (d,hidden_layer_sizes[0])) #np.ones((d,hidden_layer_sizes[0]))\n",
    "    weights = []\n",
    "    weights.append(weight_layer) #append(0.1*weight_layer)\n",
    "    \n",
    "    #Initializing the weights for hidden layers\n",
    "    for l in range(L-3):\n",
    "        # first hidden[l] + 1 means we account for bias, \n",
    "        weight_layer = np.random.normal(0, 0.1, (hidden_layer_sizes[l]+1,hidden_layer_sizes[l+1])) \n",
    "        weights.append(weight_layer) \n",
    "\n",
    "    #Initializing the weights for output layers\n",
    "    weight_layer= np.random.normal(0, 0.1, (hidden_layer_sizes[l+1]+1,1)) \n",
    "    weights.append(weight_layer) \n",
    "    \n",
    "    for e in range(epochs):\n",
    "        choiceArray=np.arange(0, N)\n",
    "        np.random.shuffle(choiceArray)\n",
    "        errN=0\n",
    "        for n in range(N):\n",
    "            index=choiceArray[n]\n",
    "            x=np.transpose(X_train[index])\n",
    "            #TODO: Model Update: Forward Propagation, Backpropagation\n",
    "            X, S = forwardPropagation(x, weights)\n",
    "            gradList = backPropagation(X, y_train[index], S, weights)\n",
    "\n",
    "            # update the weight and calculate the error\n",
    "            weights = updateWeights(weights ,gradList, alpha)\n",
    "            errN += errorPerSample(X, y_train[index])\n",
    "        err[e]=errN/N \n",
    "    return err, weights\n",
    "\n",
    "def activation(s):\n",
    "    return s if s > 0 else 0\n",
    "\n",
    "def derivativeActivation(s):\n",
    "    return 1 if s > 0 else 0\n",
    "\n",
    "def derivativeError(x_L,y):\n",
    "    if y == 1:\n",
    "        return -1 / x_L\n",
    "    elif y == -1:\n",
    "        return 1 / (1 - x_L)\n",
    "    \n",
    "def derivativeOutput(s):\n",
    "    es = np.exp(-s)\n",
    "    ret = es / ((1 + es) ** 2)\n",
    "    return ret\n",
    "\n",
    "def outputf(s):\n",
    "    ret = 1 / (1 + np.exp(-s))\n",
    "    return ret\n",
    "\n",
    "def errorf(x_L,y):\n",
    "    if y == 1:\n",
    "        return -1 * np.log(x_L)\n",
    "    elif y == -1:\n",
    "        return -1 * np.log(1 - x_L)\n",
    "\n",
    "\n",
    "def forwardPropagation(x, weights):\n",
    "    l=len(weights)+1 # l now is layer number \n",
    "    currX = x\n",
    "    retS=[]\n",
    "    retX=[]\n",
    "    retX.append(currX)\n",
    "\n",
    "    for i in range(l-1): # we only loop l - 1 layer since the last layer \n",
    "        \n",
    "        currS= np.matmul(np.transpose(weights[i]), currX) # weights[i].shape = (a, b), curX.shape = a, c\n",
    "        retS.append(currS)\n",
    "        currX=currS\n",
    "        if i != len(weights)-1:\n",
    "            for j in range(len(currS)):\n",
    "                currX[j] = activation(currX[j])\n",
    "            currX = np.hstack((1,currX))\n",
    "        else:\n",
    "            currX = outputf(currX)\n",
    "        retX.append(currX)\n",
    "    return retX,retS\n",
    "\n",
    "\n",
    "def backPropagation(X,y_n,s,weights): \n",
    "    #x:0,1,...,L\n",
    "    #S:1,...,L\n",
    "    #weights: 1,...,L\n",
    "    l=len(X)\n",
    "    delL=[]\n",
    "\n",
    "    # To be able to complete this function, you need to understand this line below\n",
    "    # In this line, we are computing the derivative of the Loss function w.r.t the \n",
    "    # output layer (without activation). This is dL/dS[l-2]\n",
    "    # By chain rule, dL/dS[l-2] = dL/dy * dy/dS[l-2] . Now dL/dy is the derivative Error and \n",
    "    # dy/dS[l-2]  is the derivative output.\n",
    "    delL.insert(0,derivativeError(X[l-1],y_n)*derivativeOutput(s[l-2])) # We use s[l - 2] is bc X[i] is always after activation of previous s[i - 1]\n",
    "    curr=0\n",
    "    \n",
    "    # Now, let's calculate dL/dS[l-2], dL/dS[l-3],...\n",
    "    for i in range(len(X)-2, 0, -1): #L-1,...,0\n",
    "        delNextLayer=delL[curr]\n",
    "        WeightsNextLayer=weights[i]\n",
    "        sCur=s[i-1]\n",
    "        \n",
    "        #Init this to 0s vector\n",
    "        delN=np.zeros((len(s[i-1]),1))\n",
    "\n",
    "        #Now we calculate the gradient backward\n",
    "        #Remember: dL/dS[i] = dL/dS[i+1] * W(which W???) * activation\n",
    "        for j in range(len(s[i-1])): #number of nodes in layer i - 1\n",
    "            for k in range(len(s[i])): #number of nodes in layer i \n",
    "              \n",
    "                delN[j] = delN[j] + delNextLayer[k] * WeightsNextLayer[j + 1][k]\n",
    "            delN[j] = delN[j] * derivativeActivation(sCur[j])\n",
    "        delL.insert(0,delN)\n",
    "    \n",
    "    # We have all the deltas we need. Now, we need to find dL/dW.\n",
    "    # It's very simple now, dL/dW = dL/dS * dS/dW = dL/dS * X\n",
    "    g=[]\n",
    "    for i in range(len(delL)):\n",
    "        rows,cols=weights[i].shape\n",
    "        gL=np.zeros((rows,cols))\n",
    "        currX=X[i]\n",
    "        currdelL=delL[i]\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                #TODO: Calculate the gradient using currX and currdelL\n",
    "                gL[j,k] = currX[j].item() * currdelL[k].item() # I think we could just do currdelL[k] * currX[j]\n",
    "        g.append(gL)\n",
    "    return g\n",
    "\n",
    "\n",
    "def updateWeights(weights, g, alpha):\n",
    "    nW = []\n",
    "    for i in range(len(weights)):\n",
    "        rows, cols = weights[i].shape\n",
    "        currWeight = weights[i]\n",
    "        currG = g[i]\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                currWeight[j, k] = currWeight[j, k] - alpha * currG[j, k]  \n",
    "        nW.append(currWeight)\n",
    "    return nW\n",
    "\n",
    "\n",
    "def errorPerSample(X,y_n):\n",
    "    return errorf(X[len(X) - 1], y_n)\n",
    "\n",
    "\n",
    "def pred(x_n, weights):\n",
    "    retX, retS = forwardPropagation(x_n, weights) # x_n includes the dummy 1\n",
    "\n",
    "    # threshold at 0.5\n",
    "    if retX[-1] < 0.5:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def confMatrix(X_train,y_train,w):\n",
    "    eCount = np.zeros((2,2))\n",
    "    row, col = X_train.shape\n",
    "\n",
    "    # add dummy column of 1s\n",
    "    X0 = np.ones((row,1))\n",
    "    X_train = np.hstack((X0,X_train))\n",
    "\n",
    "    for j in range(row):\n",
    "        if (pred(X_train[j], w) == -1) and (y_train[j] == -1):\n",
    "            eCount[0,0] += 1\n",
    "        elif (pred(X_train[j], w) == 1) and (y_train[j] == -1): \n",
    "            eCount[0,1] += 1\n",
    "        elif (pred(X_train[j], w) == 1) and (y_train[j] == 1):\n",
    "            eCount[1,1] += 1\n",
    "        else:\n",
    "            eCount[1,0] += 1\n",
    "\n",
    "    return eCount\n",
    "\n",
    "def plotErr(e,epochs):\n",
    "    x = np.arange(1, epochs + 1)\n",
    "    y = e\n",
    "    plt.title(\"Error vs. Epoch\") \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.plot(x,y,linewidth=2.0)\n",
    "    plt.show()\n",
    "\n",
    "def test_SciKit(X_train, X_test, Y_train, Y_test):\n",
    "    model = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(30, 10), random_state=1)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    cfMatrix = confusion_matrix(Y_test, Y_pred, labels=[-1, 1])\n",
    "    return cfMatrix\n",
    "\n",
    "\n",
    "def test_Part1():\n",
    "    from sklearn.datasets import load_iris\n",
    "    X_train, y_train = load_iris(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train[50:],y_train[50:],test_size=0.2, random_state=1)\n",
    "    \n",
    "    for i in range(80):\n",
    "        if y_train[i]==1:\n",
    "            y_train[i]=-1\n",
    "        else:\n",
    "            y_train[i]=1\n",
    "    for j in range(20):\n",
    "        if y_test[j]==1:\n",
    "            y_test[j]=-1\n",
    "        else:\n",
    "            y_test[j]=1\n",
    "        \n",
    "    err,w=fit_NeuralNetwork(X_train,y_train,1e-2,[30, 10],100)\n",
    "    \n",
    "    plotErr(err,100)\n",
    "    \n",
    "    cM=confMatrix(X_test,y_test,w)\n",
    "    \n",
    "    sciKit=test_SciKit(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    print(\"Confusion Matrix is from Part 1a is: \",cM)\n",
    "    print(\"Confusion Matrix from Part 1b is:\",sciKit)\n",
    "\n",
    "test_Part1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
